\documentclass[12pt]{article}

\usepackage{geometry}
\geometry{a4paper}
\geometry{margin=2.5cm,top=2cm,bottom=2cm}

\usepackage{lastpage}
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\renewcommand{\sectionmark}[1]{ \markright{#1}{} }
\lhead{Cyril NOVEL (cbn13)}\chead{}\rhead{\textit{ \nouppercase{\rightmark}}}
\lfoot{}\cfoot{\thepage\ of \pageref{LastPage}}\rfoot{}

\setlength{\headheight}{15pt}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{stmaryrd}
\usepackage{url}
%\usepackage{hyperref}
%\usepackage[noend]{algpseudocode}

\title{OFusion\\
\large{CO542 Individual Project}}

\author{Cyril NOVEL}

\date{\today}

\begin{document}
\maketitle
\newpage

\tableofcontents

\newpage

\addcontentsline{toc}{section}{Introduction}
\section*{Introduction}
Augmented reality is a dynamic field in Computer Science and Computer Vision, with the multiplication of portable devices with high perfomance CPU and GPU. Multiples devices are providing an augmented reality experience. The Nintendo 3DS has a dedicated game with augmented reality cards, applications can be downloaded on smartphones. The Google Glass works like a small Head Up Display for the user, similar to what can be seen in some videogames. The Project Tango phone and tablet comes with built-in augmented reality applications. However, none of these devices are capable of displaying information for the entire field of view of the user. The smartphone or tablet size is small compared to the size of the field of view. The Google Glass encounters the same problem: there is no possible overlay of the reality.

By using a virtual reality headset, such as the Oculus Rift, we cover the entire field of view of the user. Using an appropriate sensor, we can understand the scene surrounding the user. Then we can overlay information on the reality for the whole field of view. This creates a new kind of augmented reality device. Applications are infinite: the road to follow can be directly highlighted in a GPS application; you can play a virtual game in your living room with the game understanding the geometry of your living room -- making each game unique since you can move objects; a medical application can highlight organs during an operation and also provide an HUD with useful information. Possibilities are countless.

During this project, we created a device fusionning an Oculus Rift and a Kinect to create a new kind of augmented reality device. The Kinect captures the surrounding scene. It is then rendered in 3D in the Oculus Rift with real-time color information. Thus the user can evolve just like in real life. An augmented reality application has been integrated. Using a \textit{magic pen}, it is possible to draw virtually on a surface.
\newpage

\section{Hardware review}
\subsection{Virtual reality headsets}
A virtual reality headsets is a device capable of displaying 3D scenes to the user wearing the headset. It is made of screen and two lenses, with the left part of the screen showing the image for the left eye and the right eye for the right part.

Virtual reality headsets have been fancied a long-time and are finally becoming reality. The Oculus Rift is the first affordable headset to encounter a commercial success. Previously, Nintendo tried to sell a virtual reality headset called the Virtual Boy. It was a failure since its use caused terrible headaches to the players and only 5 shades of red were available.

In the following sections, we will present some virtual reality headsets and highlight their differences.

\subsubsection{Oculus Rift and Sony Morpheus}
The Oculus Rift is developped by Oculus VR \cite{Oculus}. It is the first virtual headset targetting the mass market, and therefore has an aggressive price -- the first development kit costs 300\$, 350\$ for the second one.

The Oculus uses a screen for displaying the image. In the first development kit, the resolution of the screen was 1280x800, leading to an effective 640x800 per eye. One part of the screen displays the image for one eye, the other part displays the image for the other eye. The Rift uses lenses to distort the image on the screen, so that it fits the actual field of view of the human eye. The field of view of the Rift is 90 degrees horizontally. Figure~\ref{oculusback} shows the back of the Oculus Rift.

Gyros, accelerometers and magnetometers are combined in the helmet in order to track the user's head movements. The first development kit is however unable to detect translation movement.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.2]{OculusBack.jpg}
  \caption{\label{oculusback} The Oculus Rift}
\end{figure}

The major drawback of the Rift is the resolution of the screen. The screen-door effect is predominant and prevent a total immersion in the displayed scene. The second version of the Development Kit comes with a resolution of 960x1080 per eye, decreasing this effect. This next version comes also with a camera, tracking the movement of the head, in order to detect translation in addition to rotation.

The success of the Oculus Rift leads other companies to develop similar devices. Sony presented few months ago the project Morpheus \cite{Morpheus} -- shown in figure~\ref{morpheus}, highly similar to the Oculus Rift.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{Morpheus.jpg}
  \caption{\label{morpheus} Sony Morpheus}
\end{figure}

Sony Morpheus uses a comparable technology to Oculus Rift. The most noticable difference is that Morpheus has tracking technology all around it, whereas Oculus Rift only has tracking captors on the front and on the sides of the display.

\subsubsection{Google Cardboard and Samsung Gear VR}
During the Google I/O 2014, Google introduced the Cardboard Project. The idea is to use a smartphone as the screen for the headset and create the headset from a cardboard. Google presented the project as a Do-It-Yourself headset, acknowledging that the most difficult part was purchasing the lenses. The design files are available online and some online stores sell and ship the set for 25\$.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{GoogleCardboard.png}
  \caption{\label{cardboard} The Google Cardboard project}
\end{figure}

The advantage of Google Carboard is that it uses the sensors in the smartphone for the head positionning. Every recent smartphone is equiped with a gyroscope and a magnetometer, so the head tracking is easy to achieve.

Samsung is developping a similar headset called Gear VR. Little information are available at the moment but the Gear VR is based on the same idea as Google Cardboard.

The main advantage of smartphones is the high pixel density of the screen. The LG G3 offers a 538 ppi screen, when the first Oculus Rift development kit has \textit{only} a 216 ppi screen. This high density screen counters the screen door effect.

\subsubsection{Vrvana Totem}
The Totem, developed by Vrvana \cite{Vrvana}, is similar to the Oculus Rift, except that two cameras are embedded on the front of the headset -- see figure~\ref{tpg}. The idea is that each camera captures the view of one eye.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{TruePlayerGear.jpg}
  \caption{\label{tpg} The Totem, with two cameras}
\end{figure}

The lens distorsion is computed directly via the hardware in the headset. The others characteristics are higly similar to the Oculus Rift. The Totem is very interesting thanks to these embedded cameras. We want to achieve a new type of augmented reality. The Totem directly captures what the eyes would see. This guarantees an almost perfect 3D image for the user. However, the 3D reconstruction is more difficult - as seen in section~\ref{subsec:stereocam}.

\subsection{Capturing the scene}
For rendering the scene in a virtual reality headset, we need to capture the scene surrounding the user. We can not just extract features from the scene. We need dense information about the scene in real time in order to create the corresponding 3D surface. In this part we present different devices that can compute a point cloud in real time.

\subsubsection{Structured light scanning}
Structured light scanning is a technique used for computing a depth map from the projection of near IR light patterns on the scene. The device project an IR light pattern on the surface of the room. Depending on the depth of the objects, the pattern is bigger or smaller than the reference pattern. Thus we can infer a depth map by reading the projection of the IR pattern. An exemple of pattern is presented in figure~\ref{kinectir}.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{Kinect-ir-image.png}
  \caption{\label{kinectir} Kinect IR pattern}
\end{figure}

This technique is used by the first Kinect -- figure~\ref{kinect}. The resolution of the depth map is $320*240$ points. Other devices using this technique exist, such as the Asus Xtion or the PrimeSense Capri sensor.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{kinect1.jpg}
  \caption{\label{kinect} First Kinect. From left to right: IR Pattern projector, IR Camera for reading the projected pattern, RGB camera}
\end{figure}

It is now possible to create very small IR light projector and IR light captor. The Capri sensor is ten times smaller than the Kinect sensor and can be easily mounted on a tablet or a virtual reality headset. Google Project Tango, a tablet with 3D sensing capabilities, is supposed to use the PrimeSense Capri sensor for capturing 3D information.

However, the use of structured light comes with drawbacks. In very luminous area, the IR light captor can't work because the incoming light is blinding it. Surfaces like TV or computer screen reflect the IR light and so no information can be retrieve from these areas. Finally, the distance between the projector and the captor creates a shadow effect around the object. Very close objects can't be processed since they aren't properly seen by the captor or the sensor. And since the light intensity decreases with the distance, remote objects can't be seen. The computed distance is sometimes wrong for large distances (more than 2 meters). The depth distribution is discrete and coarse for large distance. Beyond two meters, the depth estimation is often considered as unreliable.

\subsubsection{Time of Flight}
A Time of Flight camera is estimating the distance of a point based on the known speed of light. The camera relies on light pulse. The illumination is switched on for a very short time on one point. The light reflects on the object and is gathered by the camera. The delay between the start of the pulse and the gathering by the camera is the time taken by the light to travel twice the distance between the camera and the object. Knowing the speed of light, we can deduce the distance of the point.

Kinect 2 is using a time of flight camera -- figure~\ref{kinect2}. The resolution is much higher than Kinect one, since Kinect 2 produces a $512*424$ depth map. The time of flight doesn't suffer from illumination problems, position problems or distance problems. The depth distribution is not coarse in the remote areas, allowing a nice accuracy of the measurements -- see figure~\ref{depthK2}.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{kinect2.jpg}
  \caption{\label{kinect2} Kinect 2}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{kinect2depth.jpg}
  \caption{\label{depthK2} Depth map with the time of flight camera of the Kinect 2}
\end{figure}

However, Time Of Flight cameras are very expensive, especially for 30Hz processing of the environment. Until recently, real-time TOF cameras had a lower resolution than the Kinect 1. Due to the nature of light reflectivity, the depth estimation is less reliable when the light ray is not perpendicular to the surface. Figure~\ref{depthK2} shows a lot of noise and holes at the extremities of the depth map.

\subsubsection{Stereo camera}
\label{subsec:stereocam}
Stereoscopy uses two or more pictures for finding corresponding features and replace them in 3D space. Given two images of the same object from slightly different points of view, we identify the features that appear on both images. This search is easily performed with the use of Computational Stereo techniques, such as the computation of the epipolar lines. Since we know where the images have been taken relatively to each others, we can compute the depth of the feature. We perform this for every features and we retrieve a point cloud with the approximate same resolution as the images taken.

The chinese company Etron is selling very small devices with two optical captors and a processor for computing the depth map. A resulting depthmap is shown in figure~\ref{etron}.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{EtronDepth.png}
  \caption{\label{etron} Depth map with two stereo camera}
\end{figure}

Stereo camera system is the cheaper system for retrieving a point cloud. It only requires some calibration and computation, but it is not a very expensive hardware compared to the other two systems.

The computed depth map is unfortunately very noisy. There is a lot of noise when we compute the depth map. Shadow effects can be seen on figure~\ref{etron}. Moreover it is very difficult to compute a distance for uniform area, since no feature information can be retrieved. We need to extrapolate the distance based  on the surrounding information, but this leads to low fidelity reconstruction of the scene.

\subsection{Conclusion}

\newpage
\section{Design/Theory}
\subsection{KinectFusion}
For the reconstruction of the scene and the localization of the user, we use the KinectFusion algorithm. The KinectFusion algorithm achieves real-time dense surface mapping and reconstruction. The algorithm is fully described in \cite{KF1, KF2}. The GPU implementation of KinectFusion allows the real time reconstruction and localization. The algorithm is composed of four main stages:
\begin{enumerate}
\item \textbf{Depth Map conversion} The live depth map is converted into absolute 3D positions and normals in the camera coordinate space;
\item \textbf{Camera tracking} We keep a rigid 6 degrees of freedom transform of the camera. At time $i$, the camera pose is noted $T_i = [R_i|t_i]$ with $R_i$ being a 3x3 rotation matrix and $t_i$ being a 3D translation vector. We use the Iterative Closest Point algorithm for tracking the position and orientation of the camera.
\item \textbf{Volumetric integration}
\item \textbf{Raycasting} The volume is raycast to render the implicit surface.
\end{enumerate}

Each of this stage is performed in parallel on the GPU using CUDA.

\subsubsection{Depth Map conversion}
The depth map is a 2D array of float. At time $i$, for each pixel $u(x,y)$ we have the corresponding depth $D_i(u)$. Given the calibration matrix $K$ of the Kinect infrared camera, we can compute the 3D position in the camera coordinate space. For each pixel $u$, a GPU thread compute the 3D position as follow:
$$v_i(u) = D_i(u)K^{-1}[u,1]$$
The results are saved in a single vertex map $V_i$.

We also compute the corresponding normal map, as it will be useful later for the ICP algorithm. Each normal $n_i(u)$ is computed by a GPU thread using the neighboring reprojected points:
$$n_i(u) = (v_i(x+1,y) - v_i(u))\times (v_i(x,y+1) - v_i(u))$$
The normal is then normalized and stored in a single normal map $N_i$.

Given the rigid body transform of the camera $T_i = [R_i|t_i]$, we can convert the vertices and the normals into global coordinates:
$$v_i^g(u) = T_iv_i(u)$$
$$n_i^g(u) = R_in_i(u)$$

\subsubsection{Camera tracking}
Iterative Closest Point algorithm was introduced by \cite{ICP1}. It is a widely studied algorithm for 3D shapes alignement. \cite{ICP2} provides a detailed study of the ICP algorithm. In KinectFusion, ICP is also used for tracking the camera pose of the Kinect, that means its position and its orientation. For each new depth frame captured by the Kinect, the algorithm estimates the 6DOF transform that aligns the vertices and normals of the current frame with those of the previous frame. This gives a relative 6DOF transform. If we apply them together incrementally, we obtain the global camera pose $T_i$.

The first part of the ICP is to find correspondences between the current oriented points set at time $i$ with the previous one at time $i-1$. KinectFusion uses \textit{projective data association}. Given the previous global rigid body transform $T_{i-1}$, each GPU thread project a point $v_{i-1}$ into camera coordinate space. It then looks up the corresponding point in $V_i$ and in $N_i$ (respectfully the current vertex map and the current normal map). Doing so, we find the corresponding points along the same ray. Next, each thread tests the corresponding points to reject outliers. We compute the euclidean distance between the two global positions and angle between the two corresponding normals. If both of them are within a threshold, the correspondence is validated. A pseudo-code of the algorithm is described in Algorithm~\ref{algo1}. $T_i$ is initialized at $T_{i-1}$ and is updated with an incremental transform at the end of each step of the ICP.

\begin{algorithm}
\caption{Projective point-plane data association}\label{algo1}
\begin{algorithmic}[1]
\For{$\text{each pixel } u \in \text{ depth map } D_i \text{ in parallel}$}
  \If{$D_i(u) > 0$}
  \State $v_i \gets T_{i-1}^{-1}v_{i-1}^g$
  \State $p \gets \text{perspective project vertex of } v_{i-1}$
    \If{$p \in \text{ vertex map } V_i$}
    \State $v \gets T_iV_i(p)$
    \State $n \gets R_iN_i(p)$
      \If{$\left\|v-v_{i-1}^{g}\right\| < \text{ distance threshold and } abs(n\cdot n_{i-1}^{g}) < \text{ normal threshold}$}
      \State point correspondence found
      \EndIf
    \EndIf
  \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

All the corresponding points are saved into the same set. The output of each ICP iteration is a transformation matrix $T$ that minimizes the point-to-plane metrics, defined as follows:
$$\text{arg min } \sum_{D_i(u)>0} \left\|(Tv_i(u)-v_{i-1}^g(u))\cdot n_{i-1}^g(u) \right\|^2$$

Assuming only an incremental transform occurs between frames, the above system can be solve using a linear approximation. The linear system is computed and summed in parallel on the GPU. The 6x6 linear system obtained is then solved on the CPU using a Cholesky decomposition.

The ICP iteration is repeated multiple times in order to obtain the most accurate rigid transform for $T_i$. This algorithm is working since $T_i$ and $T_{i-1}$ are close enough to make the first approximation on the ICP : $T_i \approx T_{i-1}$.

\subsubsection{Volumetric representation and integration}
Thanks to the ICP, we now have the global pose for each depth measurement. That way, each depth map can be converted from image coordinates to global coordinate space. For computing a 3D surface, multiple techniques exists -- \cite{ISO} provides a review of the principal methods. We need a fast computation of the 3D surface, so all the Delaunay-based or region-growing techniques can not be applied here.

Instead, we use a volumetric representation based on \cite{VolRep}. A 3D volume of fixed resolution is defined such as it maps the specific dimension of a 3D physical space. This volume is then divided into a grid of uniform voxels. Each voxel stores the a variant of Signed Distance Functions \cite{SDF}, specifying a relative distance to the underlying surface. The value of a Signed Distance Function is positive if the voxel is in front of the surface, negative otherwise. The surface interface is defined by the zero-crossing -- where the values change sign.

In KinectFusion, the value stored in a voxel is a Truncated Signed Distance Function -- TSDF \cite{VolRep}. This representation can easily be kept up to date with incoming data points -- Kinect provides roughly 5 millions measurements each second. It can handle uncertainty in the data, deal with multiple measurements and fill holes as new measurements are done.

To achieve a real time integration of the 3D vertices, KinectFusion uses a GPU implementation. The 3D voxel grid is stored on the GPU as aligned linear memory. This implementation is not memory efficient -- a $512^3$ volume with 32-bits voxels requires 512MB of memory -- but speed efficient. Due to the linear alignement, access from the GPU threads are faster because the access can be coalesced. Algorithm~\ref{algo2} describes the main steps of the GPU implementation. Because there is a large number of voxels, it is impossible to launch one thread per voxel. To ensure a coalesced memory access, each GPU thread is assigned to a $(x,y)$ position at the front of the 3D voxel grid. In parallel, GPU threads go through the volume, moving along the Z-axis. Since the resolution and the physical dimensions are known, a voxel can be converted into a 3D positions in global coordinate space. We can calculate the distance between the voxel and the camera position -- $t_i$ -- and we can project the vertex back onto the image plane to know the actual depth measured along the ray. The difference gives the new SDF value for the voxel (line 7). The SDF is then normalized to a TSDF (line 9 and 11) and averaged with the previous stored value    (line 13).

\begin{algorithm}
\caption{Projective TSDF integration}\label{algo2}
\begin{algorithmic}[1]
\For{$\text{each voxel } g \in x,y \text{ volume slice } \text{ in parallel}$}
  \While{$\text{sweeping from front slice to back}$}
  \State $v^g \gets \text{convert }g\text{ from grid to global 3D position}$
  \State $v \gets T_i^{-1}v^g$
  \State $p \gets \text{perspective project vertex} v$
    \If{$v \in \text{ camera field of view}$}
    \State $sdf_i \gets \left\|t_i-v^{g}\right\| - D_i(p)$
      \If{$sdf_i > 0$}
      \State $tsdf_i \gets \text{min}(1, sdf_i/\text{max truncation})$
      \Else
      \State $tsdf_i \gets \text{max}(-1, sdf_i/\text{min truncation})$
      \EndIf
    \State $w_i \gets \text{min}(\text{max weight}, w_{i-1}+1)$
    \State $tsdf^{avg} \gets \dfrac{tsdf_{i-1}w_{i-1}+tsdf_iw_i}{w_{i-1}+w_i}$
    \State store $w_i$ and $tsdf^{avg}$ in voxel $g$
    \EndIf
  \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}

\subsubsection{Raycasting}
Once the vertices are integrated, we need to render the 3D scene from the point of view of the camera. KinectFusion implements a GPU based raycaster. Algorithm~\ref{algo3} gives the pseudocode of the raycaster. In parallel, each GPU renders one pixel of the generated view. A GPU thread is given a starting position and walks along the ray corresponding to the pixel being rendered. The position of the implicit surface is extracted when the signum of the TSDF changes. The surface intersection point is merely linearly interpolated given the sample points on either sides of the zero-crossing. Assuming that the gradient is orthogonal to the surface, the normal is computed as the derivative of the TSDF at the zero-crossing \cite{SDF}. So each GPU thread can compute the surface point and its normal. Given a light source, it is then easy to render a simple lighting of the model.

\begin{algorithm}
\caption{Parallel raycaster and shader}\label{algo3}
\begin{algorithmic}[1]
\For{$\text{each pixel } u \in \text{ output image in parallel }$}
  \State $\text{ray}^{\text{start}} \gets \text{ back project} [u,0]; \text{ convert to grid position}$
  \State $\text{ray}^{\text{next}} \gets \text{ back project} [u,1]; \text{ convert to grid position}$
  \State $\text{ray}^{\text{dir}} \gets \text{ normalize }(\text{ray}^{\text{next}}-\text{ray}^{\text{start}})$
  \State $\text{ray}^{\text{len}} \gets 0$
  \State $g \gets \text{first voxel along } \text{ray}^{\text{dir}}$
    \While{$\text{voxel } g \text{within volume bounds}$}
    \State $\text{ray}^{\text{len}} \gets \text{ray}^{\text{len}} + 1$
    \State $g^{\text{prev}} \gets g$
    \State $g \gets \text{traverse next voxel along }\text{ray}^{\text{dir}}$
      \If{zero crossing from $g$ to $g^{\text{prev}}$}
      \State $p \gets$ extract trilinear interpolated grid position
      \State $v \gets$ convert $p$ from grid to global 3D position
      \State $n \gets$ surface gradient as $\nabla tsdf(p)$
      \State $u \gets$ correct pixel shading for oriented point $(v,n)$
      \EndIf
    \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Oculus Rift Headtracking}
\subsubsection{Calculating the orientation}
The Oculus Rift Headtracking system uses a gyroscope, an accelerometer and a magnetometer \cite{Oculus2}. Steve LaValle described in two different blog posts \cite{OculusBlog1, OculusBlog2} how the headtracking was working.

The orientation of a rigid body can be expressed with 3 angles: \textit{yaw}, \textit{pitch} and \textit{roll}. Figure~\ref{ypr} illustrates these 3 angles. While they are easy to understand and represent, these angles cause trouble due to numerical singularities -- gimbal lock \cite{lock}, and it comes with a huge numbers of incompatible definitions, regrouped under the \textit{Euler angles} name. The use of \textit{quaternions} instead overcome these problems \cite{Quat}.

The gyroscope inside the Rift measures a vector $(\omega_x,\omega_y,\omega_z)$ corresponding to the angular velocity along each axis. Since we are using quaternions, we need the angle of the rotation and the axis of the rotation. $(\omega_x,\omega_y,\omega_z)$ is the rotation axis once it is normalized. The length of $(\omega_x,\omega_y,\omega_z)$ is the angular speed of rotation along that axis.

The update of the orientation is done with the Euler Integration method. The update equation is:
$$\text{Current quaternion = Previous quaternion} * \text{Quat(axis, angle)}$$

\begin{figure}[h]
  \centering
  \includegraphics[scale=3]{OculusHead.jpg}
  \caption{\label{ypr} The 3 angles characterizing the orientation of the head}
\end{figure}

The Euler Integration is not an exact method and the gyroscope measurements are not 100\% accurate. Thus the true orientation is drifting away from the calculated orientation. Other sensors are needed to correct the orientation. The accelerometer is used for correcting the \textit{tilt error} -- where the up direction is -- while the magnetometer is used to correct the \textit{yaw error}.

\subsubsection{Correction of the tilt error}
The up direction is defined by the gravity and the acceleration vector, which is roughly $9.81m/s^2$. Using a three-axis accelerometer, we can measure this vector but also every other additional accelerations due to the movement of the headset. Since the drift is slow, two conditions have to be met before correcting the drift:
\begin{enumerate}
\item The length of the measured acceleration vector is around $9.81m/s^2$
\item The gyroscope reports that the rotation is very slow
\end{enumerate}
There are some flaws in this method. It is possible to cancel the gravity acceleration by moving down while creating a lateral acceleration of $9.81m/s^2$. However this method is simple enough and works in the vast majority of cases.

Once the acceleration vector $a$ is found, the angle $\phi$ between $a$ and the $z$-axis is computed. The rotation axis is called the tilt axis and correspond to $(-a_z, 0, a_x)$. Rotating along this axis of angle $\phi$ re-aligns the up direction correctly -- see Figure~\ref{tilt}.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.2]{tilt.png}
  \caption{\label{tilt} Visulalisation of the tilt axis}
\end{figure}

\subsubsection{Correction of the yaw error}
The yaw error is corrected by using the magnetic field of the Earth. The magnetic field of the Eart can be represented by a 3D vector field -- with magnitude and direction. Since the user of the Rift stays in the same area, the magnetic field is assumed to be constant. The magnetometer can not directly measure the absolute magnetic field $m = (m_x, m_y, m_z)$ because it measures the field relative to its own orientation. The three-axis magnetometer measures $m$ projected on each of its axes. Figure~\ref{field} illustrates the issue in 2D.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.2]{RotMag.png}
  \caption{\label{field} Measurement of the magnetic field}
\end{figure}

If the sensor is rotated anticlockwise by angle $\theta$, then the measured field is rotated by $-\theta$. So a magnetometer readind from inside the rift gives a clue about its orientation based on the sensed magnetic field vector.

However, the Earth magnetic field is not the only one present in the everyday life. All electronic devices create a magnetic field. According to the Public Health England, the exposure level due to electrical appliances is close to 0.002 Gauss, whereas the Earth magnetic field is around 0.5 Gauss in the UK \cite{HPA}. So the magnetic fields from electronic devices can be neglected, except for the Rift itself. The Rift generate a constant magnetic field vector at the sensor, so a fixed offset is applied for each magnetometer sensor axis. By turning the Rift around through all orientations, the observed values should then lie on a sphere -- after noise filtering.

The readings of the magnetometers are then used for calibrating the Rift. Once it is calibrated, the drift is detected by using reference points. To save a reference point, the Oculus stores the magnetometer reading and the estimate of the Rift orientation. To detect a yaw drift error, the algorithm uses 4 steps:
\begin{enumerate}
\item Take the reference and transform the magnetometer into a world frame vector $m_w=(m_x,m_y,m_z)$ using the orientation -- quaternion -- at which it was stored.
\item Take the current magnetometer reading and transform it into a world frame vector $r_w=(r_x,r_y,r_z)$ using the current Rift orientation.
\item $r_w$ and $m_w$ are projected on the horizontal plane -- it is the same assuming the tilt error has already been corrected.
\item $\theta = atan2(m_z,m_x)$ and $\phi = atan2(r_z,r_x)$ gives us the yaw drift error $e = \theta - \phi$ with $e \in [-\pi,\pi]$.
\end{enumerate}

Knowing the yaw error, the orientation of the Rift is ajusted accordingly.

\subsection{Head mounted display and Lens deformation}
The screen of the Rift -- sometimes called Head Mounted Display or HMD -- displays one image for the left part on the left part of the screen and one image for the right eye on the right part of the screen. The hardware is optimised for an interpupillary distance of 65mm. Two different images mean that we need to rendere the scene from two different points of view.

The lenses in the Rift magnify the images, thus providing an almost real-life field of view. However, the lenses distort the images a lot. If we display the original 3D image on the screen, then the user observes a pincushion distortion effect \cite{OVRDoc}.

\newpage
\section{Implementation}
\subsection{kfusion}
\subsection{Lens deformation}
\subsection{Color rendering}
\subsection{Oculus Calibration}
\subsection{Magic Pen}

\newpage
\section{Results}
\subsection{3D rendering}
\subsection{Magic pen}

\newpage
\section*{Conclusion}
\addcontentsline{toc}{section}{Conclusion}

\newpage
\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{biblio}
\end{document}